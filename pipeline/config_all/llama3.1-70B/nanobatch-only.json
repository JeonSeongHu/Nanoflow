{
    "model": {
        "architectures": [
            "LlamaForCausalLM"
          ],
          "attention_bias": false,
          "attention_dropout": 0.0,
          "bos_token_id": 128000,
          "eos_token_id": [
            128001,
            128008,
            128009
          ],
          "hidden_act": "silu",
          "hidden_size": 8192,
          "initializer_range": 0.02,
          "intermediate_size": 28672,
          "max_position_embeddings": 131072,
          "mlp_bias": false,
          "model_type": "llama",
          "num_attention_heads": 64,
          "num_hidden_layers": 80,
          "num_key_value_heads": 8,
          "pretraining_tp": 1,
          "rms_norm_eps": 1e-05,
          "rope_scaling": {
            "factor": 8.0,
            "low_freq_factor": 1.0,
            "high_freq_factor": 4.0,
            "original_max_position_embeddings": 8192,
            "rope_type": "llama3"
          },
          "rope_theta": 500000.0,
          "tie_word_embeddings": false,
          "torch_dtype": "bfloat16",
          "transformers_version": "4.42.3",
          "use_cache": true,
          "vocab_size": 128256
    },
    "model_configs": {
        "gpu_num": 8,
        "run_layer": 80,
        "allocate_kv_data_batch": 1480,
        "frame_page_size": 16,
        "max_batch_size": 2048,
        "gpu_mem": 68719476736,
        "page_mem_size": 32768
    },
    "pipeline_configs": {
        "gemm_op_tag": [
            "128_128_32_64_64_32_3_5_RowMajor_RowMajor_RowMajor",
            "128_128_32_64_64_32_1_4_RowMajor_RowMajor_RowMajor",
            "128_128_32_64_64_32_1_5_RowMajor_RowMajor_RowMajor",
            "128_128_32_64_64_32_1_4_RowMajor_RowMajor_RowMajor",
            "128_64_64_64_32_64_2_3_RowMajor_RowMajor_RowMajor",
            "128_128_32_64_64_32_2_5_RowMajor_RowMajor_RowMajor",
            "128_128_32_64_64_32_1_5_RowMajor_RowMajor_RowMajor",
            "128_64_64_64_32_64_2_3_RowMajor_RowMajor_RowMajor",
            "128_256_32_64_64_32_2_3_RowMajor_RowMajor_RowMajor"
            ],
        "global_batch_size": 2048,
        "nanobatch_1_size": 640,
        "kqv1_size": 256,
        "kqv3_size": 768
    },
    "serve_configs": {
        "model": "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "actual_gpu_num": 8,
        "weight_path": "./nanoflow_weight_70B_3_1/",
        "hf_path": "../../../hf/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/33101ce6ccc08fa6249c10a543ebfcac65173393",
        "pipeline_type": "NANOBATCH"
    }
  }
     